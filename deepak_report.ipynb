{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "492c6022-d717-4a7c-9320-2b9f9ce55f8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## grade.py\n",
    "\n",
    "`grade.py` receives the form scan of the form as an image and extracts the shaded responses as well as whether something was written next to a question. \n",
    "\n",
    "### Usage\n",
    "\n",
    "```bash\n",
    "python3 grade.py <form> <results>\n",
    "```\n",
    "\n",
    "Example:\n",
    "\n",
    "```bash\n",
    "python3 grade.py test-images/a-27.jpg a-27_results.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fc07ac-fdec-4908-b625-79cffcc726de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3ee331c-6e54-4e6f-b96a-c1b1881103fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Approaches\n",
    "\n",
    "We tried different ways to extract the poistion of the boxes.\n",
    "\n",
    "#### Grid Overlay\n",
    "\n",
    "The intention was to manually estimate the pixel offsets both horizontally and vertically using a test image to fit a grid over the answers portion of the form. The grid can then be used to index individual boxes to obtain access to them. The problem with this method is that it is not translation, scaling, or rotation invariant.\n",
    "The predicted grid did not fit all of the test images, as shown in the image below. \n",
    "\n",
    "<img src=\"report-resources/grid.png\" alt=\"grid\" width=\"400\"/>\n",
    "\n",
    "\n",
    "#### Pair of Horizontal and Vertical Lines\n",
    "\n",
    "Any algorithm that extracts the positions of the boxes must be translation invariant, which means it must be able to determine where the answers block begins in both horizontal and vertical directions. As a result, the Hough transform was used on a thresholded image to determine the topmost vertical and leftmost horizontal lines, which subsequently indicate where the block begins. \n",
    "\n",
    "<img src=\"report-resources/horiz-vert-lines.png\" alt=\"horiz-vert-lines\" width=\"400\"/>\n",
    "\n",
    "This method is not robust to other lines that may emerge as in the figure above, where a horizontal line appears above the answer form and might be interpreted as the starting point for the answers block. In addition, the Hough transfrom may not always find the same left and top grid lines perfectly due to the printing ink contrast. \n",
    "\n",
    "#### Vertical Patches\n",
    "\n",
    "The idea is to find the vertical lines that join the boxes and extract vertical patches having only a stack of boxes, which can then be processed further. This approach will be translation and scale invariant after being corrected for rotational tilt. This method of extracting vertical patches and recognizing shaded boxes is detailed in the following section. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7986b9d-26ec-46a9-8dd9-7dd26ae435cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Methodology\n",
    "\n",
    "- Preprocessing\n",
    "    - Use 5x5 Gaussina blur to reduce noise\n",
    "    - Invert and threshold the imgae such that pixels values less than 200 are set to 0 otherwise 255.\n",
    "- Identify and correct the tilt\n",
    "- Idetify and select prominent vertical lines\n",
    "- Extract vertical patches\n",
    "- Identify shaded boxes\n",
    "- Identify whether something was written next to questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1c2f94-27b1-45fc-b307-04c5225021b8",
   "metadata": {},
   "source": [
    "#### Hough Lines\n",
    "\n",
    "Hough transform algorithm is implemented from scratch to find the straight lines in the image. This algorithm can work with varying $\\rho$-$\\theta$ grid resolutions.\n",
    "\n",
    "\n",
    "Algorithm:\n",
    "- $\\rho$, $\\theta$ are discretized using the resolutions provided within the range $-d \\le \\rho \\le d$, $d = \\sqrt{w^2 + h^2}$ and $0 \\le \\theta < \\pi$ respectively.\n",
    "- An accumulator is grid is constructed by using dicretized $\\rho$ and $\\theta$.\n",
    "- For each non zero pixel in the input image, all possible pairs of $\\rho$ and $\\theta$ are obtained fixing (x,y) using the equation $\\rho = x cos(\\theta) + y sin(\\theta)$\n",
    "    - For each pair the corresponding grid value in the accumulator is incremented.\n",
    "- Finally, $\\rho$ and $\\theta$ for the grid cells that exceed the vote threshold given are returned.\n",
    "\n",
    "This algorithm is an improved version of [1].\n",
    "\n",
    "\n",
    "Example Image             |  Hough Space\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"report-resources/example-pentagon.png\" alt=\"example-pentagon\" width=\"400\"/>  |  <img src=\"report-resources/pentagon-hough-transform.png\" alt=\"pentagon-hough-transform\" width=\"400\"/>\n",
    "\n",
    "The image is a inverse threshold version found in [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c5db02-f991-4160-a232-8609f9e79fc3",
   "metadata": {},
   "source": [
    "#### Tilt Correction\n",
    "\n",
    "We determine the angular deviation of the lines from the horizontal and rotate the image in the opposite direction to make the method resilient to tilts in the image.\n",
    "\n",
    "\n",
    "The image is preprocessed with gaussian blur, inverse thresholding, and sobel filter to find horizontal edges.\n",
    "Lines closer to slope 90 (horizontal) are then found using the Hough transform by using a fine grid where theta resolution is set to 1/16th of degree.\n",
    "The tilt $theta$ away from the horizontal is indicated by the average slope of the selected lines.\n",
    "After that, the entire image is rotated by $-theta$ around the center. This makes lines joining boxes vertical and accessing the boxes by horizontal and vertical offset feasible. \n",
    "\n",
    "<img src=\"report-resources/tilted-top-portion.png\" alt=\"horiz-vert-lines\" width=\"800\"/>\n",
    "\n",
    "\n",
    "Original with tilt             |  After tilt correction\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"report-resources/original-with-tilt.png\" alt=\"horiz-vert-lines\" width=\"400\"/>  |  <img src=\"report-resources/after-tilt-correction.png\" alt=\"horiz-vert-lines\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a124fd98-7440-44e0-9688-b89815fb8adc",
   "metadata": {},
   "source": [
    "#### Identifying the vertical lines\n",
    "\n",
    "Before applying the Hough Transform, the image is blurred with a 5x5 gaussian filter and inverse thresholded.\n",
    "The grid is finer, with a $\\rho$ result of 1/4th of a pixel and a $\\theta$ resolution of 1/4th of a degree.\n",
    "To extract as many lines as feasible, the vote threshold is set low (300).\n",
    "The lines are then filtered to maintain only those with slopes near zero or 180 degrees.\n",
    "\n",
    "\n",
    "The vertical lines discovered after using the above procedure are depicted in the image below. \n",
    "\n",
    "\n",
    "<img src=\"report-resources/vertical-lines.png\" alt=\"vertical-lines\" width=\"400\"/>\n",
    "\n",
    "\n",
    "Because of pixels in letters (options: A B C D E) and pixels from darkened boxes, there are too many lines that pass through the boxes.\n",
    "By combining lines that are close together, these extra lines are removed.\n",
    "\n",
    "\n",
    "The merging procedure is as follows: all vertical lines are sorted using $\\rho$ so that the lines that are next to each other in the image are also side by side in the array.\n",
    "The lines are then looped over until only those that are $k$ pixels apart in $\\rho$ values are kept.\n",
    "Only the vertical lines that cross through the vertical margins of the boxes are left after this technique as it eliminates lines that are close to each other. \n",
    "\n",
    "\n",
    "<img src=\"report-resources/merged-lines.png\" alt=\"merged-lines\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c597c7-0531-4ba7-bfd0-025f3115559d",
   "metadata": {},
   "source": [
    "#### Extracting the vectical patches\n",
    "\n",
    "After the above step, 36 prominent vertical lines are extracted. Each of these lines divide the image to smaller regions of vertical patches.\n",
    "\n",
    "<img src=\"report-resources/terminilogy.png\" alt=\"terminilogy\" width=\"200\"/>\n",
    "\n",
    "The questions are arranged into 3 \"stacks\". Each _stack_ contains 3 \"substacks\" (showed in the image using dotted lines), the first _substack_ contains the written characters, the question numbers are in the second and the third thas the boxes.\n",
    "\n",
    "The third _substack_ is further divided into 5 columns \"col\"s that has options A, B, C, D, E in that order. Using these three paramters _stack_, _substack_, and _col_, a particular vertical patch can be extracted.\n",
    "\n",
    "These paramters are zero indexed, _stack_, _substack_ has 0,1,2 values, and _col_ has 0,1,2,3,4 values.\n",
    "\n",
    "For example, the options B vertical patch of questions 30-58 is extraced using \n",
    "\n",
    "```\n",
    "stack = 1\n",
    "substack = 2\n",
    "col=1\n",
    "```\n",
    "\n",
    "<img src=\"report-resources/vertical-patch.png\" alt=\"vertical-patch\" width=\"100\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0bdd8b-902d-4964-8c46-77d283080171",
   "metadata": {},
   "source": [
    "#### Identifying the shaded boxes\n",
    "\n",
    "To identify the shaded boxes as well as to extract the start and end positions of boxes in the vertical patch.\n",
    "\n",
    "For a vertical patch extracted in the previous section, if the pixel values in the x - direction were averaged, a signal of length equal to height of the image is formed. This signal is plotted in the below image, y-axis represents signal strength and x-axis corresponds to vertical offset in the image.\n",
    "\n",
    "\n",
    "<img src=\"report-resources/avg-intensity.png\" alt=\"avg-intensity\"/>\n",
    "\n",
    "We can discren the following from this graph\n",
    "\n",
    "- The shaded box will have intensities higher than unshaded boxes.\n",
    "- The gaps between the boxes can be seen where the signal strength is close to zero.\n",
    "- Height of a box corresponds to width of the signal that is non-zero\n",
    "- Spurious lines coming from unexpected lines in the image or noisy Hough transform can be elimiated by considering the portions in the graph where the signal is non zero and has a certain width.\n",
    "\n",
    "<img src=\"report-resources/avg-intensity-cropped.png\" alt=\"avg-intensity-cropped\" width=\"600\"/>\n",
    "\n",
    "\n",
    "An approach to segment the boxes and identify whether shaded or not:\n",
    "1. Using two-pass connected component labelling algorithm we segment the signal. \n",
    "2. Eliminate components whose width is smaller than $w$ pixels.\n",
    "3. Average the signal in each segment, if the average is greater than threshold $s$, then mark that segment as shaded.\n",
    "\n",
    "\n",
    "<img src=\"report-resources/connected-components.png\" alt=\"connected-components\"/>\n",
    "\n",
    "We identify the shaded boxes for each vertical patch in each stack, which tells us which choice is shaded for each question.\n",
    "All shaded responses for all questions can be identified by maintaining an option accumulator for each question and looping through all vertical patches. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860ee980-47cc-4f49-a62a-ba6f2ff7ddb2",
   "metadata": {},
   "source": [
    "#### Identifying the written characters\n",
    "\n",
    "The edges of the segmented components represents the horizontal sides of the boxes, (`y_start`, `y_end`). Using this information and _substack_=1 vertical patch, the space where written characters exists can be extracted for each question.\n",
    "\n",
    "If the average intensity of this small patch is greater than a certain threshold, then it is assumed that there was something written in this patch. \n",
    "\n",
    "Reducing this threshold will increase false-poitives because of noise or the presence of box edges leaking over from the adjacent vertical patches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b83f588-19b5-4cef-bf06-4e26e3c876f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### References\n",
    "\n",
    "\n",
    "[1]: https://alyssaq.github.io/2014/understanding-hough-transform/ (Understanding Hough Transform With Python)\n",
    "\n",
    "[2]: https://moonbooks.org/Articles/Implementing-a-simple-python-code-to-detect-straight-lines-using-Hough-transform/\n",
    "\n",
    "[3]: https://en.wikipedia.org/wiki/Connected-component_labeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
